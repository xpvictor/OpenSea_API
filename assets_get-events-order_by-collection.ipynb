{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from decimal import Decimal, InvalidOperation\n",
    "\n",
    "pd.options.display.float_format = '{:,.14f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() #add external key request for user access\n",
    "\n",
    "slug_collection = 'boredape-baseclub'\n",
    "initial_token = 'LWV2ZW50X3RpbWVzdGFtcD0yMDI1LTAyLTExKzEyJTNBMTQlM0EzNi40NzQ4ODkmLWV2ZW50X3R5cGU9Y29sbGVjdGlvbl9vZmZlciYtcGs9MzE5Nzc0MTM2Nzk=' #first pagination \n",
    "event_type = 'order'\n",
    "limit_request = 50\n",
    "\n",
    "url = f'https://api.opensea.io/api/v2/events/collection/{slug_collection}?event_type={event_type}&limit={limit_request}'\n",
    "api_key = os.getenv('api_key')\n",
    "\n",
    "headers = {\n",
    "  \"accept\": \"application/json\",\n",
    "    \"x-api-key\": api_key  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Creating first request without pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_response = requests.get(url, headers=headers)\n",
    "data = first_response.json()\n",
    "\n",
    "datasource_dir = \"datasource\"\n",
    "orders_dir = os.path.join(datasource_dir,\"orders\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"events_batch_0_{timestamp}.json\"\n",
    "\n",
    "if not os.path.exists(datasource_dir):\n",
    "    os.makedirs(datasource_dir)\n",
    "if not os.path.exists(orders_dir):\n",
    "    os.makedirs(orders_dir)\n",
    "\n",
    "filepath = os.path.join(orders_dir,filename)\n",
    "\n",
    "with open(filepath,\"w\") as f:\n",
    "    json.dump(data,f)\n",
    "\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Creating all struture to collect all orders request with pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_request(url, pagination_token, headers): \n",
    "    try:\n",
    "        response = requests.get(url + f'&next={pagination_token}', headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error request: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(batch_size=2, delay=10,max_requests=50): #batch_process function is responsible for run requests in batch, according to interval\n",
    "    base_dir = \"datasource\"\n",
    "    orders_dir = os.path.join(base_dir, \"orders\")\n",
    "\n",
    "    #Creating filepath\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        print(f\"Pasta '{base_dir}' criada.\")\n",
    "    #Creating filepath\n",
    "    if not os.path.exists(orders_dir):\n",
    "        os.makedirs(orders_dir)\n",
    "        print(f\"Pasta '{base_dir}' criada.\")\n",
    "\n",
    "    current_token = initial_token\n",
    "    batch_number = 1\n",
    "    total_paginas_processadas = 0\n",
    "\n",
    "    #Looping condition: While having 'asset_event' into request the function will create new requests, updating the pagination token\n",
    "    #For each new request the result will be adding into \"all_event\" list \n",
    "    #When the last pagination is the same as \"current_token\" OR  don't having pagination the function break\n",
    "    #All request will become JSON files and will be saved. Those file will be used into Power BI project\n",
    "    while True:\n",
    "        all_events = []\n",
    "        request_count = 0\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            data = main_request(url,current_token, headers)\n",
    "            if not data or 'asset_events' not in data:\n",
    "                print(\"End of data. Error in request\")\n",
    "                return\n",
    "            \n",
    "            all_events.extend(data['asset_events'])\n",
    "            next_token = data.get('next', None)\n",
    "\n",
    "            if not next_token or next_token == current_token:\n",
    "                print(\"There no pagination avaliable\")\n",
    "                if all_events:\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    filename = f\"events_batch_{batch_number}_{timestamp}.json\"\n",
    "                    filepath = os.path.join(orders_dir,filename)\n",
    "                    with open(filepath, 'w') as f:\n",
    "                        json.dump(all_events, f)\n",
    "                    print(f\"✅ Final batch saved in {filename} (Events: {len(all_events)})\")\n",
    "                return  \n",
    "\n",
    "            current_token = next_token\n",
    "            request_count += 1\n",
    "            total_paginas_processadas += 1\n",
    "\n",
    "            # Break after X requests\n",
    "            if request_count % max_requests == 0:\n",
    "                print(f\"⏸ Breack of {delay} segunds...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "        # Saving the request as JSON file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"events_batch_{batch_number}_{timestamp}.json\"\n",
    "        filepath = os.path.join(orders_dir, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(all_events, f)\n",
    "        print(f\"✅ Batch {batch_number} saved in {filepath} (Events: {len(all_events)})\")\n",
    "\n",
    "        batch_number += 1\n",
    "        time.sleep(delay)  \n",
    "\n",
    "batch_process(batch_size=2, delay=10, max_requests=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_json = 'datasource/orders'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "#making validation to collect only json files and adding into dataframes list \n",
    "for arquivo in os.listdir(pasta_json):\n",
    "    if arquivo.endswith('.json'):\n",
    "        caminho_completo = os.path.join(pasta_json, arquivo)\n",
    "\n",
    "        df = pd.read_json(caminho_completo)\n",
    "        dataframes.append(df)\n",
    "\n",
    "df_orders_raw = pd.concat(dataframes, ignore_index= True)\n",
    "\n",
    "#Creating dataframes through dictionaries series\n",
    "df_asset = df_orders_raw['asset'].apply(pd.Series)\n",
    "df_payment = df_orders_raw['payment'].apply(pd.Series)\n",
    "df_criteria = df_orders_raw['criteria'].apply(pd.Series)\n",
    "df_criteria_collection = df_criteria['collection'].apply(pd.Series)\n",
    "df_criteria_contract = df_criteria['contract'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adding datas from dataframes above into main dataframe\n",
    "df_orders = pd.concat([\n",
    "    df_orders_raw.drop(columns=['asset','payment','criteria']),\n",
    "    df_asset.add_prefix('asset_'),\n",
    "    df_payment.add_prefix('payment_'),\n",
    "    df_criteria.add_prefix('criteria_'),\n",
    "    df_criteria_collection.add_prefix('criteria-collection_'),\n",
    "    df_criteria_contract.add_prefix('criteria-collection_')\n",
    "],axis=1)\n",
    "\n",
    "\n",
    "df_orders['start_date'] = pd.to_datetime(df_orders['start_date'],unit='s')\n",
    "df_orders['start_date'] = df_orders['start_date'].dt.strftime('%d-%m-%Y')\n",
    "df_orders['expiration_date'] = pd.to_datetime(df_orders['expiration_date'],unit='s')\n",
    "df_orders['expiration_date'] = df_orders['expiration_date'].dt.strftime('%d-%m-%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders['payment_quantity_characters'] = df_orders['payment_quantity'].astype(str).str.len()\n",
    "df_orders['payment_qtd'] = df_orders['payment_quantity'].astype(str).str[:12]\n",
    "df_orders['payment_qtd'] = pd.to_numeric(df_orders['payment_qtd'], errors='coerce')\n",
    "\n",
    "\n",
    "def calcular_payment_value(row):\n",
    "    qtd_value = row['payment_quantity_characters'] - row['payment_decimals']\n",
    "    if row['payment_quantity_characters'] > row['payment_decimals']:\n",
    "        return int(str(row['payment_qtd'])[:qtd_value])\n",
    "    elif row['payment_quantity_characters'] < row['payment_decimals']:\n",
    "        return float(row['payment_qtd']/10**12)\n",
    "\n",
    "df_orders['payment_value'] = df_orders.apply(calcular_payment_value,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_dir = \"datasource\"\n",
    "orders_dir = os.path.join(datasource_dir,\"orders\")\n",
    "filename = f\"{slug_collection}_{event_type}.csv\"\n",
    "\n",
    "if not os.path.exists(datasource_dir):\n",
    "    os.makedirs(datasource_dir)\n",
    "if not os.path.exists(orders_dir):\n",
    "    os.makedirs(orders_dir)\n",
    "\n",
    "filepath = os.path.join(orders_dir,filename)\n",
    "\n",
    "df_orders.to_csv(filepath, index=False, encoding=\"utf-8\")                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
